{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitterAPI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0637092ffa6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwitterAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitterAPI'"
     ]
    }
   ],
   "source": [
    "#UNDER CONSTRUCTION\n",
    "\n",
    "import os\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "import pandas_datareader\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tweepy\n",
    "import twitterAPI\n",
    "import sklearn\n",
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as vaderSA\n",
    "import tqdm\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "path = \"/Users/Matzekoek/Documents/Webhose.io - World News Articles sep-oct 2015/Webhose.io - World News Articles Sep 2015(2)\"\n",
    "os.chdir(path)\n",
    "\n",
    "alphaVantageKey = '6AEOHIB29K740TG5'\n",
    "twitterapikey = 'jNjZ10xjmy3f3pMTojP9kjNWY'\n",
    "twitterapisecret = 'szou2LYaqQPXE9XMEKYnA3ETz2NZoHZcHuylswb7mgJxXCWPIE'\n",
    "twitteraccesstoken = '1126856371989884929-m4K4b5Bm40yzIkHTr714a2wMRF5WaZ'\n",
    "twitteraccesstokensecret = '49P9ZAs2w42IqJ059Fl5ylVvinnuF4wIQPP12xnDxxSHW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program design:\n",
    "\n",
    "\n",
    "## NEWS\n",
    "\n",
    "#access Twitter and download all FB tweets\n",
    "#store news with headline, content, date, category(, no of newssources?, virality?)\n",
    "#clean news data (universal time, correct for stock market closing, correct for weekends, remove unwanted characters from text)\n",
    "#analyze sentiment of news content\n",
    "\n",
    "## STOCK DATA\n",
    "\n",
    "#access EOD stock data for FB stock from NYSE\n",
    "#for specific time/date, predict if up/down on basis of sum of valence for that day\n",
    "#(or weighted by # of sources/category/virality etc)\n",
    "\n",
    "## IDEAS\n",
    "#integrate NY weather data\n",
    "#query twitter for keywords from headlines, and analyze sentiment\n",
    "#widely covered, very viral, very negative news data can have negative effect on stock market (terror attacks, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "#access intraday stock data for NYSE\n",
    "#for specific time/date, predict if up/down on basis of sum of valence for that hour/10min \n",
    "#(or weighted by # of sources/category/virality etc)\n",
    "\n",
    "#DONE:\n",
    "\n",
    "#Get news data\n",
    "#store news with headline, content, date, category, no of newssources?, virality?\n",
    "#clean news data (universal time, correct for news after trading closes, remove unwanted characters from text)\n",
    "#analyze sentiment of content\n",
    "#extract timezone, check for abberations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitterapikey = \n",
    "# twitterapisecret = \n",
    "# twitteraccesstoken = \n",
    "# twitteraccesstokensecret =  \n",
    "\n",
    "CONSUMER_KEY = 'jNjZ10xjmy3f3pMTojP9kjNWY'\n",
    "CONSUMER_SECRET = 'szou2LYaqQPXE9XMEKYnA3ETz2NZoHZcHuylswb7mgJxXCWPIE'\n",
    "ACCESS_KEY = '1126856371989884929-m4K4b5Bm40yzIkHTr714a2wMRF5WaZ'\n",
    "ACCESS_SECRET = '49P9ZAs2w42IqJ059Fl5ylVvinnuF4wIQPP12xnDxxSHW'\n",
    "\n",
    "auth = tweepy.auth.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "search_results = api.search(q=\"facebook\", count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dflist = []\n",
    "\n",
    "# for i in search_results:\n",
    "#     dflist.append(i)\n",
    "\n",
    "# pp.pprint(search_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdf = pd.read_csv('Webhose.io - World News Articles Sep 2015.csv')\n",
    "\n",
    "#newsdf.to_csv('Webhose.io - World News Articles Sep 2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare files for processing\n",
    "\n",
    "files = []\n",
    "for filename in os.listdir(path):\n",
    "    files.append(str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get relevant news features\n",
    "\n",
    "def getNewsFeatures(functionfile):\n",
    "    \n",
    "    jsonfile = json.load(open(str(functionfile)))\n",
    "    \n",
    "    date =           jsonfile['published']\n",
    "    title =          jsonfile['title']\n",
    "    source =         jsonfile['thread']['site']\n",
    "    source_section = jsonfile['thread']['section_title']\n",
    "    language =       jsonfile['language']\n",
    "    country =        jsonfile['thread']['country']\n",
    "    text =           jsonfile['text']\n",
    "    return [date, title, source, source_section, language, country, text]\n",
    "    #pp.print (date, title, source, source_section, language, country, text)\n",
    "\n",
    "#samplefile = 'news_0000001.json'\n",
    "#samplejson = json.load(open('news_0000001.json'))\n",
    "#pp.pprint(getNewsFeatures(samplefile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert raw data to dataframe\n",
    "\n",
    "columns = ['date', 'title', 'source','source_section','language','country','text']\n",
    "\n",
    "newsdf = pd.DataFrame(columns = columns)\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    jsondf = pd.DataFrame(getNewsFeatures(file)).T\n",
    "    jsondf.columns = columns\n",
    "    newsdf = newsdf.append(jsondf)\n",
    "    \n",
    "\n",
    "newsdf.reset_index(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct news timestamp for: timezone, published after closed trading, weekend\n",
    "\n",
    "newsdf['datetime'] = pd.to_datetime(newsdf['date'].str[0:19], format='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "utcdiff = datetime.timedelta(hours=-7)\n",
    "newsdf['datetime_utc_corrected'] = newsdf['datetime'] + utcdiff\n",
    "newsdf['date_1700_utc'] = pd.to_datetime(newsdf['datetime_utc_corrected'].astype(str).str[0:10] + ' ' + '17:00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "daydelta = datetime.timedelta(days = 1)\n",
    "mask = newsdf.datetime_utc_corrected > newsdf.date_1700_utc\n",
    "column_name = 'datetime_utc_corrected'\n",
    "newsdf.loc[mask, column_name] = (newsdf['datetime_utc_corrected'].apply(lambda dt: dt.replace(hour=9, minute=0)) + daydelta)\n",
    "\n",
    "twodaydelta = datetime.timedelta(days=2)\n",
    "newsdf['date_weekday'] = newsdf.datetime_utc_corrected.apply(lambda dt: dt.weekday())\n",
    "\n",
    "mask = newsdf.date_weekday == 5\n",
    "column_name = 'datetime_utc_corrected'\n",
    "newsdf.loc[mask, column_name] = (newsdf['datetime_utc_corrected'] + twodaydelta)\n",
    "\n",
    "mask = newsdf.date_weekday == 6\n",
    "column_name = 'datetime_utc_corrected'\n",
    "newsdf.loc[mask, column_name] = (newsdf['datetime_utc_corrected'] + daydelta)\n",
    "\n",
    "\n",
    "newsdf['text'] = newsdf['text'].str.replace(r'\\n', ' ')\n",
    "newsdf['text'] = newsdf['text'].str.replace(\"'\", ' ')\n",
    "newsdf['text'] = newsdf['text'].str.replace('\"', ' ')\n",
    "newsdf['text'] = newsdf['text'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract compound sentiment score with VADER, create sentiment dataframe, add to newsdf\n",
    "\n",
    "analyser = vaderSA()\n",
    "\n",
    "textlist = newsdf['text'].tolist()\n",
    "sentimentdf = pd.DataFrame(columns = ['text_compound_sentiment_score'])\n",
    "\n",
    "sentimentlist = []\n",
    "for text in textlist:\n",
    "    sentimentlist.append(analyser.polarity_scores(str(text))['compound'])\n",
    "    \n",
    "sentimentdf = pd.DataFrame(sentimentlist, columns = ['text_compound_sentiment'])\n",
    "\n",
    "newsdf['text_compound_sentiment'] = sentimentdf['text_compound_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Finance Data from Alpha Vantage\n",
    "\n",
    "#ts = TimeSeries(key='alphaVantageKey', output_format='pandas')\n",
    "#stockdf, meta_data = ts.get_daily_adjusted(symbol='FB', outputsize='full')\n",
    "\n",
    "#stockdf[(stockdf['date']>startdate) & (stockdf['date']<enddate)]\n",
    "\n",
    "# print (data)\n",
    "# data['5. adjusted close'].plot()\n",
    "# plt.title('Stock chart')\n",
    "# plt.show()\n",
    "\n",
    "startdate = '2015-09-01'\n",
    "enddate = '2015-09-30'\n",
    "\n",
    "stockdf = stockdf[startdate:enddate]\n",
    "\n",
    "stockdf['up'] = stockdf['5. adjusted close'] > stockdf['1. open']\n",
    "stockdf['down'] = stockdf['5. adjusted close'] < stockdf['1. open']\n",
    "\n",
    "\n",
    "\n",
    "# print (stockdf)\n",
    "# stockdf['5. adjusted close'].plot()\n",
    "# plt.title('Stock chart')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "getNewsFeatures(samplefile)\n",
    "\n",
    "\n",
    "path = \"/Users/Matzekoek/Documents/Webhose.io - World News Articles sep-oct 2015/Webhose.io - World News Articles Sep 2015\"\n",
    "os.chdir(path)\n",
    "\n",
    "columns = ['date', 'title', 'source','source_section','language','country','text', 'sentiment']\n",
    "\n",
    "newsdf = pd.DataFrame(columns = columns)\n",
    "\n",
    "\n",
    "textlist = newsdf['text'].tolist()\n",
    "sentimentdf = pd.DataFrame(columns = ['text_compound_sentiment_score'])\n",
    "\n",
    "sentimentlist = []\n",
    "for text in textlist:\n",
    "    sentimentlist.append(analyser.polarity_scores(str(text))['compound'])\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "pp.pprint(samplejson['published'])\n",
    "pp.pprint(samplejson['title'])\n",
    "pp.pprint(samplejson['thread']['site'])\n",
    "pp.pprint(samplejson['thread']['section_title'])\n",
    "pp.pprint(samplejson['language'])\n",
    "pp.pprint(samplejson['thread']['country'])\n",
    "pp.pprint(samplejson['text'])\n",
    "\n",
    "\n",
    "#s = pd.Series(getNewsFeatures(file), index=df.columns)\n",
    "    \n",
    "testdf = pd.DataFrame(getNewsFeatures(samplefile)).T\n",
    "testdf.columns = columns\n",
    "\n",
    "samplefile2 = 'news_0000002.json'\n",
    "\n",
    "testdf2 = pd.DataFrame(getNewsFeatures(samplefile2)).T\n",
    "testdf2.columns = columns\n",
    "\n",
    "testdf = testdf.append(testdf2)\n",
    "testdf.head()  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# year = str(newsdf.iloc[0:1]['date'])[5:9]\n",
    "# month = str(newsdf.iloc[0:1]['date'])[11:12]\n",
    "# day = str(newsdf.iloc[0:1]['date'])[13:15]\n",
    "# hour = str(newsdf.iloc[0:1]['date'])[16:18]\n",
    "# minute = str(newsdf.iloc[0:1]['date'])[19:21]\n",
    "# secs = str(newsdf.iloc[0:1]['date'])[22:24]\n",
    "\n",
    "#date = str(newsdf.iloc[0:1]['date'])[5:15]\n",
    "#time = str(newsdf.iloc[0:1]['date'])[16:24]\n",
    "\n",
    "#newsdf['singledate'] = str(newsdf.iloc[0:1]['date'])[5:15]\n",
    "# FOUT   newsdf['time'] = (str(newsdf['date'])[16:24])\n",
    "\n",
    "print(newsdf['datetime'].iloc[0:1].item())\n",
    "utcdif = datetime.timedelta(hours=-5)\n",
    "\n",
    "print(newsdf['datetime'].iloc[0:1].item() + utcdif)\n",
    "\n",
    "\n",
    "#print(file)\n",
    "#soup = BeautifulSoup(open(str(os.path.join(path,file))))\n",
    "\n",
    "#newsdf['date_extract'] = newsdf['date'].str[0:10]\n",
    "#newsdf['time_extract'] = newsdf['date'].str[11:19]\n",
    "#newsdf['datetime_string'] = newsdf['date_extract'] + ' ' + newsdf['time_extract']\n",
    "\n",
    "twodaydelta = datetime.timedelta(days=2)\n",
    "newsdf['date_weekday'] = newsdf.datetime_utc_corrected.apply(lambda dt: dt.weekday())\n",
    "\n",
    "mask = newsdf.date_weekday == 5\n",
    "column_name = 'datetime_utc_corrected'\n",
    "newsdf.loc[mask, column_name] = (newsdf['datetime_utc_corrected'] + twodaydelta)\n",
    "                                 \n",
    "mask = newsdf.date_weekday == 6\n",
    "column_name = 'datetime_utc_corrected'\n",
    "newsdf.loc[mask, column_name] = (newsdf['datetime_utc_corrected'] + daydelta)\n",
    "\n",
    "\n",
    "newsdf[newsdf['datetime_utc_corrected'] > newsdf['date_1700_utc']]['datetime_utc_corrected'] = \n",
    "\n",
    "# twodaydelta = datetime.timedelta(days=2)\n",
    "# newsdf['date_weekday'] = newsdf.datetime_utc_corrected.weekday()\n",
    "\n",
    "#newsdf['datetime_1700'] = pd.to_datetime((newsdf['date_extract'] + ' ' + '17:00:00'), format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#newsdf['utc_difference_extract'] = newsdf['date'].str[23:29]\n",
    "\n",
    "#newsdf['text_compound_sentiment'] = analyser.polarity_scores(str(newsdf['text']))['compound']\n",
    "# werkt niet, geeft waarde voor hele textbody van alles samen, zelfde als analyser.polarity_scores(str(newsdf['text'].iloc[:]))\n",
    "#newsdf['text_compound_sentiment'] = newsdf['text'].apply(analyser.polarity_scores())\n",
    "# for text in newsdf['text']:\n",
    "#     newsdf['text_compound_sentiment'] = analyser.polarity_scores(text)\n",
    "\n",
    "#checked with newsdf['utc_difference_extract'].value_counts(), all from same timestamp\n",
    "\n",
    "#newsdf.to_csv('Webhose.io - World News Articles Sep 2015.csv')\n",
    "\n",
    "#fbstockdata = quandl.get(\"EOD/FB\", start_date=startdate, end_date=enddate)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
